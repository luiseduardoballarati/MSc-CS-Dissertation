{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPgwqIPA2ZmSWRIFf/HXw2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiseduardoballarati/MSc-CS-Dissertation/blob/main/The_Guardian_Scraper_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The news scraper\n",
        "\n",
        "Collecting information from the Guardian, using its API. The data come from https://open-platform.theguardian.com. The information collected can be only used for academic purposes and you need to have an API Key (you can request one on the website).\n",
        "\n",
        "Below, the necessary imports (I've used Google Colabs Notebooks):"
      ],
      "metadata": {
        "id": "ESxZa2S6EB92"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY_94R8-748Q"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The funtion *fetch_guardian_data* takes the API Key, the tags of the articles, the dates and the page size and page. It returns in json format the article type, sectionId, sectionName, the date, the title, the url and the content.\n",
        "\n",
        "It then converts to a pandas dataframe, printing the Error (Error 400 indicates that the maximumn amount of articles one can scrape per day was hit), the number of articles collected, the time taken and the date range. Other errors might indicate sucess as well. Check their documentation to understand: https://open-platform.theguardian.com/documentation/.  "
      ],
      "metadata": {
        "id": "XA7hcimAEix8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "api_key = 'YOUR_API_KEY_HERE'\n",
        "\n",
        "tag = 'business/business'\n",
        "from_date = '2015-02-23'\n",
        "to_date = '2016-02-25'\n",
        "page_size = 50  # Number of articles per page\n",
        "\n",
        "# Function to fetch data from the Guardian API\n",
        "def fetch_guardian_data(api_key, tag, from_date, to_date, page, page_size):\n",
        "    url = f'https://content.guardianapis.com/search?tag={tag}&from-date={from_date}&to-date={to_date}&show-fields=bodyText&page={page}&page-size={page_size}&api-key={api_key}'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "all_results = []\n",
        "page = 1\n",
        "\n",
        "while True:\n",
        "    data = fetch_guardian_data(api_key, tag, from_date, to_date, page, page_size)\n",
        "    if data:\n",
        "        results = data.get('response', {}).get('results', [])\n",
        "        if not results:\n",
        "            break\n",
        "        all_results.extend(results)\n",
        "        page += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Extract the desired fields from the collected results\n",
        "extracted_data = [\n",
        "    {\n",
        "        'type': item.get('type'),\n",
        "        'sectionId': item.get('sectionId'),\n",
        "        'sectionName': item.get('sectionName'),\n",
        "        'webPublicationDate': item.get('webPublicationDate'),\n",
        "        'webTitle': item.get('webTitle'),\n",
        "        'webUrl': item.get('webUrl'),\n",
        "        'content': item.get('fields', {}).get('bodyText')\n",
        "    }\n",
        "    for item in all_results\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(extracted_data)\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken to scrape the articles: {elapsed_time} seconds\")\n",
        "print(f\"Articles scrapped: {df.shape[0]}\")\n",
        "print(f\"Dates: from: {df['webPublicationDate'].min()} to {df['webPublicationDate'].max()}\")\n",
        "df.to_csv('business_guardian_articles_11.csv', index=False)"
      ],
      "metadata": {
        "id": "DHL9LQq679OL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef18888a-8999-4763-aeae-e874662ff042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 400\n",
            "Time taken to scrape the articles: 175.75619220733643 seconds\n",
            "Articles scrapped: 10302\n",
            "Dates: from: 2015-02-23T00:01:00Z to 2016-02-25T22:04:15Z\n"
          ]
        }
      ]
    }
  ]
}