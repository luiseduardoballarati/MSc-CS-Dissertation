{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qcLAE_lPRYsk",
        "mS3Zwt-9Rd4r"
      ],
      "mount_file_id": "1B3hH4P1GQhwciIShJKUOcAds1s6Drk7n",
      "authorship_tag": "ABX9TyOkeHbLaTd+XJm2M+0SDCwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiseduardoballarati/MSc-CS-Dissertation/blob/main/svo_function_GH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "This was the notebook used to generate, over a dataset of articles, a dataset containing the svo's of the articles."
      ],
      "metadata": {
        "id": "E8eTahNux-bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and dataset prep"
      ],
      "metadata": {
        "id": "qcLAE_lPRYsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-S5rNnIkHSi",
        "outputId": "8864afb3-db2a-4543-c7c2-0590025a7b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.3)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.10)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.0.4)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.4.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.25.2)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.7.5)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.66.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2024.7.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.8)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.12.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy~=3.0->textacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.12.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.0->textacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.0->textacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy~=3.0->textacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (0.1.2)\n",
            "Installing collected packages: pyphen, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.12.3 floret-0.10.5 pyphen-0.15.0 textacy-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OYgvtVTQAxY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from datetime import datetime\n",
        "import spacy\n",
        "import plotly.express as px\n",
        "from collections import defaultdict\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Bath/Dissertation/The Guardian Datasets/business_guardian_articles_3.csv')"
      ],
      "metadata": {
        "id": "IxiPSapAQG0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['webUrl'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "aADsH75jQKZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "miqc52NJjkub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy\n",
        "\n",
        "The series of functions used to extract the svo's."
      ],
      "metadata": {
        "id": "mS3Zwt-9Rd4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_processor(text):\n",
        "    # Process the text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # List to hold the results for each sentence\n",
        "    nested_list = []\n",
        "\n",
        "    # Iterate over the sentences in the document\n",
        "    for sent in doc.sents:\n",
        "        lista = []\n",
        "        # Extract dependencies for each token in the sentence\n",
        "        for token in sent:\n",
        "            lista.extend([(token.text, token.pos_, token.dep_, token.head.text)])\n",
        "\n",
        "        # Filter out stopwords and punctuation\n",
        "        filtered_data = [\n",
        "            (word1, pos, dep, word2) for word1, pos, dep, word2 in lista\n",
        "            if not (nlp.vocab[word1].is_stop or nlp.vocab[word1].is_punct)\n",
        "        ]\n",
        "        # Add the filtered data to the nested list\n",
        "        nested_list.append(filtered_data)\n",
        "\n",
        "    #return nested_list\n",
        "    return nested_list"
      ],
      "metadata": {
        "id": "_bovtnxoQNBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get node attributes\n",
        "def get_node_attributes(graph, node):\n",
        "    pos = graph.nodes[node].get('pos', 'N/A')\n",
        "    dep = graph.nodes[node].get('dep', 'N/A')\n",
        "    head = graph.nodes[node].get('head', 'N/A')\n",
        "    return pos, dep, head\n",
        "\n",
        "def graph_generator(text):\n",
        "  li = []\n",
        "  for i in range(len(text)):\n",
        "  # Create a directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # First, create a dictionary to map words to their dependencies\n",
        "    dep_map = {word: dep for word, pos, dep, head in text[i]}\n",
        "\n",
        "    # Add edges to the graph and add node attributes for POS, Dependency, Head, and Head Dependency\n",
        "    for source, pos, dep, target in text[i]:\n",
        "        head_dep = dep_map.get(target, 'N/A')\n",
        "        G.add_node(source, pos=pos, dep=dep, head=target, head_dep=head_dep)\n",
        "        G.add_node(target)  # Ensure the target node is also added\n",
        "        G.add_edge(source, target, label=dep)\n",
        "\n",
        "    # Find all weakly connected components in the graph\n",
        "    components = list(nx.weakly_connected_components(G))\n",
        "\n",
        "    components_infos = []\n",
        "    # Print each component with node attributes\n",
        "    for component in components:\n",
        "        component_info = []\n",
        "        for node in component:\n",
        "            pos, dep, head= get_node_attributes(G, node)\n",
        "            component_info.append((node, pos, dep, head))\n",
        "        components_infos.append(component_info)\n",
        "\n",
        "    li.append(components_infos)\n",
        "  return li"
      ],
      "metadata": {
        "id": "XOoMSWZRQXxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svo_generator(component_info):\n",
        "    svos = []\n",
        "\n",
        "    for sentence in component_info:  # Loop over sentences\n",
        "        for i in range(len(sentence)):  # Loop over components in each sentence\n",
        "            subjects_adj = []\n",
        "            verbs_adj = []\n",
        "            objects_adj = []\n",
        "            svo = []\n",
        "\n",
        "            subject = ''\n",
        "            verb = ''\n",
        "            objectt = ''\n",
        "\n",
        "            # check if there is a verb:\n",
        "            if not any(pos == 'VERB' for word, pos, dep, head in sentence[i]):\n",
        "              continue\n",
        "\n",
        "            # first case: nsubj + root + dobj\n",
        "            for word, pos, dep, head in sentence[i]:\n",
        "                if (dep == 'nsubj'):\n",
        "                    subject = word\n",
        "                if pos == 'VERB' and dep == 'ROOT':\n",
        "                    verb = word\n",
        "                if pos == 'NOUN' and dep == 'dobj':\n",
        "                    objectt = word\n",
        "\n",
        "            # nsubj but no ROOT\n",
        "            if verb == '':\n",
        "              for word, pos, dep, head in sentence[i]:\n",
        "                  if pos == 'VERB':\n",
        "                      verb = word\n",
        "\n",
        "           # nsubj, verb but no dobj\n",
        "            if objectt == '':\n",
        "              for word, pos, dep, head in sentence[i]:\n",
        "                  if pos == 'NOUN' and (dep == 'pobj' or dep == 'iobj'):\n",
        "                      objectt = word\n",
        "\n",
        "            for word, pos, dep, head in sentence[i]:\n",
        "                if (head == subject and pos != 'VERB') or ((pos == 'PROPN' and head != objectt and word != subject and (dep != 'pobj' or dep == 'dobj')) or (pos == 'PRON' and head != objectt and word != subject and (dep != 'pobj' or dep == 'dobj'))):\n",
        "                    subjects_adj.append(word)\n",
        "                if head == verb and pos not in ('NOUN', 'PROPN', 'PRON') and head != word:\n",
        "                    verbs_adj.append(word)\n",
        "                if head == objectt and pos != 'VERB' or dep == 'dobj':\n",
        "                      objects_adj.append(word)\n",
        "\n",
        "            # second case: no subject found:\n",
        "            if subject == '':\n",
        "                verb = ''\n",
        "                objectt = ''\n",
        "                subjects_adj = []\n",
        "                verbs_adj = []\n",
        "                objects_adj = []\n",
        "\n",
        "                for word, pos, dep, head in sentence[i]:\n",
        "                  if pos == 'NOUN' and (dep == 'pobj' or dep == 'dobj' or dep == 'iobj') and subject == '':\n",
        "                    subject = word\n",
        "                  if pos == 'VERB':\n",
        "                      verb = word\n",
        "                  if pos == 'NOUN' and word != subject and head != subject:\n",
        "                      objectt = word\n",
        "                #print(f'sub: {subject},v: {verb},o: {objectt}')\n",
        "                for word, pos, dep, head in sentence[i]:\n",
        "                  #if head == subject and pos != 'VERB' or ((pos == 'PROPN' or pos == 'PRON') and pos != 'VERB'):\n",
        "                  if head == subject and pos != 'VERB':\n",
        "                      subjects_adj.append(word)\n",
        "                  if head == verb and pos != 'NOUN' and head != word and word not in verbs_adj:\n",
        "                      verbs_adj.append(word)\n",
        "                  if (head == objectt) and (pos != 'VERB') and (word not in objects_adj) and (word not in subjects_adj) and (word != subject):\n",
        "                      objects_adj.append(word)\n",
        "\n",
        "            #print(f'sub adj: {subjects_adj},v adj: {verbs_adj},o adj: {objects_adj}')\n",
        "\n",
        "            if subjects_adj:\n",
        "                merged_subject = \" \".join(subjects_adj) + \" \" + subject\n",
        "            else:\n",
        "                merged_subject = subject\n",
        "\n",
        "            if verbs_adj:\n",
        "                if verb == 'said':\n",
        "                  merged_verbs = \" \".join(verbs_adj)\n",
        "                else:\n",
        "                  merged_verbs = verb + \" \" + \" \".join(verbs_adj)\n",
        "            else:\n",
        "                merged_verbs = verb\n",
        "\n",
        "            if objects_adj:\n",
        "                merged_objects = \" \".join(objects_adj) + \" \"\n",
        "            else:\n",
        "                merged_objects = objectt\n",
        "\n",
        "            svo = (merged_subject, merged_verbs, merged_objects)\n",
        "            svos.append(svo)\n",
        "\n",
        "    return svos"
      ],
      "metadata": {
        "id": "Zlg4VXZnTftg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svo_creator(content):\n",
        "  nested_list = spacy_processor(content)\n",
        "  lista = graph_generator(nested_list)\n",
        "  svo_list = svo_generator(lista)\n",
        "  return svo_list"
      ],
      "metadata": {
        "id": "umY8bCj8RoL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "def svo_df_creator(df):\n",
        "  df.dropna(inplace=True)\n",
        "  df['svo'] = df['content'].progress_apply(lambda x: svo_creator(x))\n",
        "  df.drop(['type', 'sectionId', 'webTitle', 'content'], axis=1, inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "0Hk9ri5XdMsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "svo_df_business_guardian_3 = svo_df_creator(df)\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken to process the sentences: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IIkzhLleCfC",
        "outputId": "305070fb-dcf1-4b56-d927-378261edd25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [03:22<00:00,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to process the sentences: 202.34005546569824 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svo_df_business_guardian_3.to_csv('svo_df_business_guardian_3.csv', index=False)"
      ],
      "metadata": {
        "id": "bv3MYZwqhx8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}